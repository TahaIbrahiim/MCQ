{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyautogui\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hand = mp.solutions.hands\n",
    "\n",
    "myhand = mp_hand.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret: break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = myhand.process(image_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for handlandmark in results.multi_hand_landmarks:\n",
    "            print(handlandmark)\n",
    "            mp_drawing.draw_landmarks(frame, handlandmark, mp_hand.HAND_CONNECTIONS)\n",
    "\n",
    "            thumb_tip = handlandmark.landmark[mp_hand.HandLandmark.THUMB_TIP]\n",
    "            index_finger_tip = handlandmark.landmark[mp_hand.HandLandmark.INDEX_FINGER_TIP]\n",
    "            middle_finger_tip = handlandmark.landmark[mp_hand.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "            ring_finger_tip = handlandmark.landmark[mp_hand.HandLandmark.RING_FINGER_TIP]\n",
    "            pinky_tip = handlandmark.landmark[mp_hand.HandLandmark.PINKY_TIP]\n",
    "\n",
    "            is_hand_close = (\n",
    "                index_finger_tip.y > thumb_tip.y and \n",
    "                middle_finger_tip.y > thumb_tip.y and\n",
    "                ring_finger_tip.y > thumb_tip.y and \n",
    "                pinky_tip.y > thumb_tip.y\n",
    "            )\n",
    "\n",
    "            if is_hand_close:\n",
    "                pass\n",
    "            else:\n",
    "                pyautogui.press('space')\n",
    "                time.sleep(0.1)\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('CV', cv2.flip(frame,1))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(\n",
    "    IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "volume.GetMute()\n",
    "volume.GetMasterVolumeLevel()\n",
    "volumeRange = volume.GetVolumeRange()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Class to represent a draggable rectangle\n",
    "class DragRect():\n",
    "    def __init__(self, posOrigin, size=[100, 100]):\n",
    "        self.posOrigin = posOrigin\n",
    "        self.size = size\n",
    "        self.dragging = False\n",
    "        self.offsetX = 0\n",
    "        self.offsetY = 0\n",
    "\n",
    "    def update(self, cursor):\n",
    "        ox, oy = self.posOrigin\n",
    "        w, h = self.size\n",
    "\n",
    "        # Check if cursor is inside the rectangle\n",
    "        if ox < cursor[0] < ox + w and oy < cursor[1] < oy + h:\n",
    "            self.dragging = True\n",
    "            self.offsetX = cursor[0] - ox\n",
    "            self.offsetY = cursor[1] - oy\n",
    "\n",
    "        # Move the rectangle if dragging\n",
    "        if self.dragging:\n",
    "            self.posOrigin = cursor[0] - self.offsetX, cursor[1] - self.offsetY\n",
    "\n",
    "    def draw(self, img):\n",
    "        ox, oy = self.posOrigin\n",
    "        w, h = self.size\n",
    "        cv2.rectangle(img, (ox, oy), (ox + w, oy + h), (0, 255, 0), 2)\n",
    "\n",
    "# Create a draggable rectangle\n",
    "rect = DragRect([100, 100], [200, 200])\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Extract landmarks for index finger tip and middle finger tip\n",
    "        index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "        middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "        \n",
    "        # Calculate distance between index finger tip and middle finger tip\n",
    "        distance = math.sqrt((index_tip.x - middle_tip.x)**2 + (index_tip.y - middle_tip.y)**2)\n",
    "        \n",
    "        # If distance is long, update rectangle position\n",
    "        if distance > 0.1:  # Adjust threshold as needed\n",
    "            cursor = int(index_tip.x * image.shape[1]), int(index_tip.y * image.shape[0])\n",
    "            rect.update(cursor)\n",
    "\n",
    "    # Draw the draggable rectangle\n",
    "    rect.draw(image)\n",
    "\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import cvzone\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HandDetector' object has no attribute 'findPosition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18296\\2772426674.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindHands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mlmList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindPosition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlmList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HandDetector' object has no attribute 'findPosition'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "detector = HandDetector(detectionCon=0.8)\n",
    "colorR = (255, 0, 255)\n",
    "\n",
    "cx, cy, w, h = 100, 100, 200, 200\n",
    "\n",
    "class DragRect():\n",
    "    def __init__(self, posCenter, size=[200, 200]):\n",
    "        self.posCenter = posCenter\n",
    "        self.size = size\n",
    "\n",
    "    def update(self, cursor):\n",
    "        cx, cy = self.posCenter\n",
    "        w, h = self.size\n",
    "\n",
    "        if cx - w // 2 < cursor[0] < cx + w // 2 and \\\n",
    "                cy - h // 2 < cursor[1] < cy + h // 2:\n",
    "            self.posCenter = cursor\n",
    "\n",
    "rectList = []\n",
    "for x in range(5):\n",
    "    rectList.append(DragRect([x * 250 + 150, 150]))\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame from the webcam\")\n",
    "        break\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = detector.findHands(img)\n",
    "    lmList, _ = detector.findPosition(img)\n",
    "    \n",
    "\n",
    "    if lmList:\n",
    "        l, _, _ = detector.findDistance(8,12, img, draw=False)\n",
    "        print(\"Distance:\", l)\n",
    "        \n",
    "        if l < 30:\n",
    "            cursor = lmList[8]\n",
    "            for rect in rectList:\n",
    "                rect.update(cursor)\n",
    "\n",
    "    imgNew = np.zeros_like(img, np.uint8)\n",
    "    for rect in rectList:\n",
    "        cx, cy = rect.posCenter\n",
    "        w, h = rect.size\n",
    "        cv2.rectangle(imgNew, (cx - w // 2, cy - h // 2),\n",
    "                      (cx + w // 2, cy + h // 2), colorR, cv2.FILLED)\n",
    "        cvzone.cornerRect(imgNew, (cx - w // 2, cy - h // 2, w, h), 20, rt=0)\n",
    "\n",
    "    out = img.copy()\n",
    "    alpha = 0.5\n",
    "    mask = imgNew.astype(bool)\n",
    "    out[mask] = cv2.addWeighted(img, alpha, imgNew, 1 - alpha, 0)[mask]\n",
    "\n",
    "    cv2.imshow(\"Image\", out)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2dede74f6ac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    " \n",
    "video_capture = cv2.VideoCapture(0)\n",
    " \n",
    "jobs_image = face_recognition.load_image_file(\"photos/jobs.jpg\")\n",
    "jobs_encoding = face_recognition.face_encodings(jobs_image)[0]\n",
    " \n",
    "ratan_tata_image = face_recognition.load_image_file(\"photos/tata.jpg\")\n",
    "ratan_tata_encoding = face_recognition.face_encodings(ratan_tata_image)[0]\n",
    " \n",
    "sadmona_image = face_recognition.load_image_file(\"photos/sadmona.jpg\")\n",
    "sadmona_encoding = face_recognition.face_encodings(sadmona_image)[0]\n",
    " \n",
    "tesla_image = face_recognition.load_image_file(\"photos/tesla.jpg\")\n",
    "tesla_encoding = face_recognition.face_encodings(tesla_image)[0]\n",
    " \n",
    "known_face_encoding = [\n",
    "jobs_encoding,\n",
    "ratan_tata_encoding,\n",
    "sadmona_encoding,\n",
    "tesla_encoding\n",
    "]\n",
    " \n",
    "known_faces_names = [\n",
    "\"jobs\",\n",
    "\"ratan tata\",\n",
    "\"sadmona\",\n",
    "\"tesla\"\n",
    "]\n",
    " \n",
    "students = known_faces_names.copy()\n",
    " \n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "s=True\n",
    " \n",
    " \n",
    "now = datetime.now()\n",
    "current_date = now.strftime(\"%Y-%m-%d\")\n",
    " \n",
    " \n",
    " \n",
    "f = open(current_date+'.csv','w+',newline = '')\n",
    "lnwriter = csv.writer(f)\n",
    " \n",
    "while True:\n",
    "    _,frame = video_capture.read()\n",
    "    small_frame = cv2.resize(frame,(0,0),fx=0.25,fy=0.25)\n",
    "    rgb_small_frame = small_frame[:,:,::-1]\n",
    "    if s:\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame,face_locations)\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            matches = face_recognition.compare_faces(known_face_encoding,face_encoding)\n",
    "            name=\"\"\n",
    "            face_distance = face_recognition.face_distance(known_face_encoding,face_encoding)\n",
    "            best_match_index = np.argmin(face_distance)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_faces_names[best_match_index]\n",
    " \n",
    "            face_names.append(name)\n",
    "            if name in known_faces_names:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (10,100)\n",
    "                fontScale              = 1.5\n",
    "                fontColor              = (255,0,0)\n",
    "                thickness              = 3\n",
    "                lineType               = 2\n",
    " \n",
    "                cv2.putText(frame,name+' Present', \n",
    "                    bottomLeftCornerOfText, \n",
    "                    font, \n",
    "                    fontScale,\n",
    "                    fontColor,\n",
    "                    thickness,\n",
    "                    lineType)\n",
    "\n",
    "                if name in students:\n",
    "                    students.remove(name)\n",
    "                    print(students)\n",
    "                    current_time = now.strftime(\"%H-%M-%S\")\n",
    "                    lnwriter.writerow([name,current_time])\n",
    "    cv2.imshow(\"attendence system\",frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    " \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting face_recognition\n",
      "  Using cached face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy in d:\\education\\bfcai 3rd\\2nd term\\neural network\\sections\\coding\\lib\\site-packages (from face_recognition) (1.24.4)\n",
      "Requirement already satisfied: Pillow in d:\\education\\bfcai 3rd\\2nd term\\neural network\\sections\\coding\\lib\\site-packages (from face_recognition) (9.2.0)\n",
      "Collecting dlib>=19.7\n",
      "  Using cached dlib-19.24.4.tar.gz (3.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: Click>=6.0 in d:\\education\\bfcai 3rd\\2nd term\\neural network\\sections\\coding\\lib\\site-packages (from face_recognition) (8.0.4)\n",
      "Collecting face-recognition-models>=0.3.0\n",
      "  Using cached face_recognition_models-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: colorama in d:\\education\\bfcai 3rd\\2nd term\\neural network\\sections\\coding\\lib\\site-packages (from Click>=6.0->face_recognition) (0.4.5)\n",
      "Building wheels for collected packages: dlib\n",
      "  Building wheel for dlib (pyproject.toml): started\n",
      "  Building wheel for dlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build dlib\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for dlib (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [13 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      Traceback (most recent call last):\n",
      "        File \"D:\\education\\BFCAI 3rd\\2nd Term\\Neural Network\\Sections\\Coding\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "          return _run_code(code, main_globals, None,\n",
      "        File \"D:\\education\\BFCAI 3rd\\2nd Term\\Neural Network\\Sections\\Coding\\lib\\runpy.py\", line 87, in _run_code\n",
      "          exec(code, run_globals)\n",
      "        File \"D:\\education\\BFCAI 3rd\\2nd Term\\Neural Network\\Sections\\Coding\\Scripts\\cmake.exe\\__main__.py\", line 4, in <module>\n",
      "      ModuleNotFoundError: No module named 'cmake'\n",
      "      \n",
      "      ERROR: CMake must be installed to build dlib\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for dlib\n",
      "ERROR: Could not build wheels for dlib, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "%pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
